# Introduction

Morsels is a client-side search library and user-interface that depends on a **pre-built index** generated by its **cli build tool**.

The difference with other search libraries like lunr.js is that this tool is focused on providing more **scalability** (but not infinitely so) using a pre-built index split into many small files. (versus other tools that provide this in a monolithic format)


## What

The core idea of this tool is to split up a monolithic postings list into many smaller files (hence the name *"Morsels"*), organised by the indexed terms. Multiple index files are batched into the same file, keeping to `16383` bytes (configurable) as much as possible.

On the client, supporting information (e.g. dictionary, document lengths, field weights) is retrieved on startup, which is usually less than a few MB even for fairly large collections.

The index files containing searched terms will be requested only on-demand from a static file server.


## Limits

The practicality / scalability of this tool is mainly bound by the following 2 factors as such.

### Size of the Largest Index Chunk Retrieved

While the index is split into many chunks, some chunks may exceed the "split size" of `16383` bytes at times. This occurs when the chunk contains a very common term (e.g. a stop word like "the"). While the information for this term could be further split into multiple chunks, all such chunks will still inevitably have to be retrieved when the term is searched, diminishing its benefit.

Certain [indexing options](./indexing_configuration.md) like removing positions and pre-caching larger chunks on startup are available to alleviate this to some extent, though not infinitely.

#### Estimations

As a rough estimate from testing, this library should be able to handle collections < `800mb` with positional indexing. Without it, the index shrinks 3-4 fold, making it potentially possible to index collections `~2gb` in size.


### Hardware Capabilities

Device capabilities is also another concern (performance when ranking and populating results), although in practice, you should be hitting limits due to the first factor (network transfer times) long before experiencing issues with this.


## Use Cases

In short, this tool is tailored for a very specific audience:
- You have a fairly large collection of html, csv, or json (only these are supported for now) files that cannot be monolithically retrieved feasibly
- You don't want or can't run a search server / search Saas (eg. Algolia Docsearch)
- You don't want the user downloading and loading a gigantic `> 50mb` index, blowing up memory usage everytime they visit your site.
- You want a complete, end-to-end and customisable search UI and file indexing solution.

> ⚠️ 
> It is **not possible** to use morsels for client-side indexing + searching as such since the indexer is a cli tool.
>
> If this is the use case, consider other lighter-weight libraries like lunr.js that already fit well.


## Libraries

This project is made up of 3 crates and 2 packages, which may be referred to in the subsequent sections of the documentation.

#### Rust crates:
- **morsels_indexer**: the cli tool providing indexing functionalities for several file formats
- **morsels_search**: internal rust wasm crate, used by the **@morsels/search-lib** package below.
- **morsels_common**: internal library containing common functionalities for the above 2 crates.

#### Npm packages:
- **@morsels/search-ui**: interfaces with @morsels/search-lib to provide basic search UI functionalities (e.g. SERP generation)
- **@morsels/search-lib**: a small companion library to morsels_search for interfacing with the wasm crate. This may be used without the `@morsels/search-ui` package in the future. For now, it only serves to separate some concerns from the UI package.
- **@morsels/lang-XX**: internal tokenizer packages for different languages generated by wasm pack.
